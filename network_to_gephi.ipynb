{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import igraph as ig\n",
    "import cairocffi as cairo\n",
    "\n",
    "\n",
    "def get_data(decade):\n",
    "    df = pd.read_csv(f\"./songs_cleaned_decade/songs_{decade}.csv\", sep=\";\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_network(data, min_count=50, min_connection=50, save_neo4j_config=False):\n",
    "    \"\"\"\n",
    "    Given a suitable dataframe, constructs a network with the following properties:\n",
    "\n",
    "    edges:\n",
    "        - size: how often do two words appear in the same song?\n",
    "    nodes:\n",
    "        - size: in how many songs does the word appear?\n",
    "        - color: degree of the node\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # read all unique words\n",
    "    print(\"Reading words...\")\n",
    "    words = []\n",
    "    for lyrics in data[\"lyrics\"]:\n",
    "        for word in lyrics.split(\" \"):\n",
    "            words.append(word)\n",
    "\n",
    "    # get unique words with counts, sort by most popular and remove words with counts < min_count\n",
    "    print(\"Word preprocessing...\")\n",
    "    words, w_counts = np.unique(words, return_counts=True)\n",
    "    words = [x for _, x in sorted(zip(w_counts, words))][::-1]\n",
    "    w_counts = sorted(w_counts)[::-1]\n",
    "    w_cutoff = w_counts.index(min_count-1)\n",
    "    words = words[:w_cutoff]\n",
    "    w_counts = w_counts[:w_cutoff]\n",
    "\n",
    "    # read song-word links\n",
    "    word_to_index = {word: i for i, word in enumerate(words)}\n",
    "    index_to_word = {i: word for i, word in enumerate(words)}\n",
    "    song_word_connector = np.zeros((len(data), len(words)))\n",
    "    for i, lyrics in enumerate(data[\"lyrics\"]):\n",
    "        for word in lyrics.split(\" \"):\n",
    "            try:\n",
    "                song_word_connector[i, word_to_index[word]] = 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "    # read all edges between remaining words\n",
    "    print(\"Reading edges...\")\n",
    "    edge_dict = {}\n",
    "    for row in song_word_connector:\n",
    "        ind1 = [i for i, x in enumerate(row) if x == 1]\n",
    "        pairs = list(itertools.combinations(ind1, 2))\n",
    "        for i, j in pairs:\n",
    "            edge_dict.setdefault(f\"{i},{j}\", 0)\n",
    "            edge_dict[f\"{i},{j}\"] += 1\n",
    "\n",
    "    # adding unique edges with thei respective count to the edges list if the count is bigger/equal min_connection\n",
    "    print(\"Edge preprocessing...\")\n",
    "    edges = []\n",
    "    for k in list(edge_dict.keys()):\n",
    "        i, j = k.split(\",\")\n",
    "        i = int(i)\n",
    "        j = int(j)\n",
    "        if edge_dict[k] >= min_connection:\n",
    "            edges.append((index_to_word[i], index_to_word[j], {\"weight\": edge_dict[k]}))\n",
    "\n",
    "    # add nodes and edges to graph\n",
    "    print(\"Adding nodes to the network...\")\n",
    "    G.add_nodes_from([\n",
    "        (word, {\"color\": \"red\", \"size\": w_counts[i]}) for i, word in enumerate(words)\n",
    "    ])\n",
    "    print(\"Adding edges to the network...\")\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    if save_neo4j_config:\n",
    "        with open(\"neo4j_config.csv\", \"w\") as f:\n",
    "            f.write(\"start,stop,weight\\n\")\n",
    "            for edge in edges:\n",
    "                f.write(f\"{edge[0]},{edge[1]},{edge[2]['weight']}\\n\")\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1950\n",
      "Reading words...\n",
      "Word preprocessing...\n",
      "Reading edges...\n",
      "Edge preprocessing...\n",
      "Adding nodes to the network...\n",
      "Adding edges to the network...\n",
      "('love', 'time', {'weight': 197})\n",
      "('love', {'color': 'red', 'size': 557})\n",
      "1960\n",
      "Reading words...\n",
      "Word preprocessing...\n",
      "Reading edges...\n",
      "Edge preprocessing...\n",
      "Adding nodes to the network...\n",
      "Adding edges to the network...\n",
      "('love', 'never', {'weight': 1616})\n",
      "('love', {'color': 'red', 'size': 4180})\n",
      "1970\n",
      "Reading words...\n",
      "Word preprocessing...\n",
      "Reading edges...\n",
      "Edge preprocessing...\n",
      "Adding nodes to the network...\n",
      "Adding edges to the network...\n",
      "('love', 'way', {'weight': 1287})\n",
      "('love', {'color': 'red', 'size': 3144})\n",
      "1980\n",
      "Reading words...\n",
      "Word preprocessing...\n",
      "Reading edges...\n",
      "Edge preprocessing...\n",
      "Adding nodes to the network...\n",
      "Adding edges to the network...\n",
      "('love', 'time', {'weight': 1313})\n",
      "('love', {'color': 'red', 'size': 2549})\n",
      "1990\n",
      "Reading words...\n",
      "Word preprocessing...\n",
      "Reading edges...\n",
      "Edge preprocessing...\n",
      "Adding nodes to the network...\n",
      "Adding edges to the network...\n",
      "('love', 'time', {'weight': 1265})\n",
      "('love', {'color': 'red', 'size': 2156})\n",
      "2000\n",
      "Reading words...\n",
      "Word preprocessing...\n",
      "Reading edges...\n",
      "Edge preprocessing...\n",
      "Adding nodes to the network...\n",
      "Adding edges to the network...\n",
      "('love', 'time', {'weight': 965})\n",
      "('love', {'color': 'red', 'size': 1874})\n",
      "2010\n",
      "Reading words...\n",
      "Word preprocessing...\n",
      "Reading edges...\n",
      "Edge preprocessing...\n",
      "Adding nodes to the network...\n",
      "Adding edges to the network...\n",
      "('love', 'time', {'weight': 1114})\n",
      "('love', {'color': 'red', 'size': 2260})\n",
      "2020\n",
      "Reading words...\n",
      "Word preprocessing...\n",
      "Reading edges...\n",
      "Edge preprocessing...\n",
      "Adding nodes to the network...\n",
      "Adding edges to the network...\n",
      "('love', 'time', {'weight': 515})\n",
      "('love', {'color': 'red', 'size': 968})\n"
     ]
    }
   ],
   "source": [
    "for decade in range(1950, 2023, 10):\n",
    "    print(decade)\n",
    "    data = get_data(decade)\n",
    "    G = build_network(data)\n",
    "\n",
    "    print(list(G.edges(data=True))[0])\n",
    "    print(list(G.nodes(data=True))[0])\n",
    "    with open(f\"gephi_data/edge_list_{decade}.csv\", \"w\") as f:\n",
    "        f.write(\"Source;Target;Label;Weight\\n\")\n",
    "        for edge in list(G.edges(data=True)):\n",
    "            f.write(f\"{edge[0]};{edge[1]};{edge[0]}-{edge[1]};{edge[2]['weight']}\\n\")\n",
    "    with open(f\"gephi_data/node_list_{decade}.csv\", \"w\") as f:\n",
    "        f.write(\"Id;Label;Name;Size\\n\")\n",
    "        for node in list(G.nodes(data=True)):\n",
    "            f.write(f\"{node[0]};{node[0]};{node[0]};{node[1]['size']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netscience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d6fe78df442a2821680c44ba2bb6e12dd7f31a25202dbe2c5c118e24ea90652"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

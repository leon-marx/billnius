{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f5ae6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\gooog\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gooog\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, wordnet, stopwords\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "959723a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lemmatize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_to_wordnet(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "    \n",
    "# list with stopwords and punctuation to remove + manually created list of non-meaningful words\n",
    "insignificant_words = ['embed', 'likeembed','might','also','like','lyric','know','go','say','oh',\n",
    "                       'ooh','get','well','come','make','one', 'yeah', 'ay','ai','see',\n",
    "                       'take','na','ca','let','tell','gon','wan',\"``\",'...', \"'s'\",\"n't\", \"'m'\", \"'cause'\"]\n",
    "stoplist = set(stopwords.words('english') + list(punctuation) + insignificant_words)\n",
    "\n",
    "def clean_lyrics(lyrics):\n",
    "    # change everything to lower case\n",
    "    lyrics = lyrics.lower()\n",
    "    # remove numbers\n",
    "    lyrics_nonum = re.sub(r'\\d+', '', lyrics)\n",
    "    \n",
    "    #tokenize the lyrics and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(lyrics_nonum))  \n",
    "    \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_to_wordnet(x[1])), nltk_tagged)\n",
    "    lemmatized_lyrics = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_lyrics.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_lyrics.append(lemmatizer.lemmatize(word, tag))\n",
    "            \n",
    "    unique_tokens = unique_tokens = list(set(lemmatized_lyrics))\n",
    "    \n",
    "    # remove stopwords\n",
    "    unique_nostop = [word for word in unique_tokens if word not in stoplist]\n",
    "    unique_nostop = [word for word in unique_nostop if not \"'\" in word]\n",
    "    return unique_nostop\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "021c15a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022\n",
      "2021\n",
      "2020\n",
      "2019\n",
      "2018\n",
      "2017\n",
      "2016\n",
      "2015\n",
      "2014\n",
      "2013\n",
      "2012\n",
      "2011\n",
      "2010\n",
      "2009\n",
      "2008\n",
      "2007\n",
      "2006\n",
      "2005\n",
      "2004\n",
      "2003\n",
      "2002\n",
      "2001\n",
      "2000\n",
      "1999\n",
      "1998\n",
      "1997\n",
      "1996\n",
      "1995\n",
      "1994\n",
      "1993\n",
      "1992\n",
      "1991\n",
      "1990\n",
      "1989\n",
      "1988\n",
      "1987\n",
      "1986\n",
      "1985\n",
      "1984\n",
      "1983\n",
      "1982\n",
      "1981\n",
      "1980\n",
      "1979\n",
      "1978\n",
      "1977\n",
      "1976\n",
      "1975\n",
      "1974\n",
      "1973\n",
      "1972\n",
      "1971\n",
      "1970\n",
      "1969\n",
      "1968\n",
      "1967\n",
      "1966\n",
      "1965\n",
      "1964\n",
      "1963\n",
      "1962\n",
      "1961\n",
      "1960\n",
      "1959\n",
      "1958\n",
      "1957\n",
      "1956\n",
      "1955\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"songs_cleaned\", exist_ok=True)\n",
    "for year in range(1955, 2023, 1)[::-1]:\n",
    "    print(year)  \n",
    "    path_songs = f\"songs/songs_{year}\"\n",
    "    songlist = [song for song in os.listdir(path_songs) if os.path.isfile(os.path.join(path_songs, song))]\n",
    "    songlist\n",
    "    df_songs = pd.DataFrame(columns = ['title','artist'])\n",
    "    song_lyrics = []\n",
    "\n",
    "    for song in songlist:  \n",
    "        # getting title and artist from the file name\n",
    "        title_artist = pd.DataFrame(song[:-4].split('by',1))\n",
    "        title_artist = title_artist.transpose()\n",
    "        title_artist.columns = ['title','artist']\n",
    "            \n",
    "        # clean lyrics\n",
    "        source = open(os.path.join(path_songs, song), 'r', encoding='cp1252')\n",
    "        lyrics = source.read()\n",
    "        clean_tokens = clean_lyrics(lyrics)\n",
    "        lyrics_string = ' '.join(clean_tokens)\n",
    "        song_lyrics.append(lyrics_string)\n",
    "        \n",
    "        # add to the dataframe\n",
    "        df_songs = pd.concat([df_songs, title_artist])\n",
    "\n",
    "    df_songs['filename'] = songlist\n",
    "    df_songs['lyrics'] = song_lyrics\n",
    "\n",
    "    # print(df_songs.head())\n",
    "    df_songs.to_csv(f\"songs_cleaned/songs_{year}.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea4b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common(df):\n",
    "    #df = pd.read_csv(f\"songs_cleaned/songs_{year}.csv\", sep=\";\")\n",
    "    words = []\n",
    "    for i, lyrics in enumerate(df[\"lyrics\"]):\n",
    "        for word in lyrics.split(\" \"):\n",
    "            # print(word)\n",
    "            words.append(word)\n",
    "    words, counts = np.unique(words, return_counts=True)\n",
    "    words = [x for _, x in sorted(zip(counts, words))][::-1]\n",
    "    counts = sorted(counts)[::-1]\n",
    "    # for i in range(len(words)):\n",
    "    counter = 0\n",
    "    for i in range(len(words)):\n",
    "        if not \"'\" in words[i]:\n",
    "            print(f\"{words[i]:20} {counts[i]}\")\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f068641",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_common(df_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcac0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = {}\n",
    "network_key = 0\n",
    "for index, row in df_songs.iterrows():\n",
    "    combined_list = [word for word in str.split(row[\"lyrics\"], \" \")]\n",
    "    #itertool product creates Cartesian product of each element in the combined list\n",
    "    for pair in itertools.product(combined_list, combined_list):\n",
    "        #exclude self-loops and count each pair only once because our graph is undirected and we do not take self-loops into account\n",
    "        if pair[0]!=pair[1] and not(pair[::-1] in network):\n",
    "            network.setdefault(pair,0)\n",
    "            network[pair] += 1 \n",
    "    \n",
    "network_df = pd.DataFrame.from_dict(network, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2745f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df.reset_index(inplace=True)\n",
    "network_df.columns = [\"pair\",\"weight\"]\n",
    "network_df.sort_values(by=\"weight\",inplace=True, ascending=False)\n",
    "network_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6beb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get weighted graph we need a list of 3-element tuplels (u,v,w) where u and v are nodes and w is a number representing weight\n",
    "up_weighted = []\n",
    "for edge in network:\n",
    "    #we can filter edges by weight by uncommenting the next line and setting desired weight threshold\n",
    "    #if(network[edge])>1:\n",
    "    up_weighted.append((edge[0],edge[1],network[edge]))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(up_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe732c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(G.nodes()))\n",
    "print(len(G.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eccf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/edgelist.csv\"\n",
    "nx.write_weighted_edgelist(G, filename, delimiter=\",\")\n",
    "#add header with appropriate column names (works on collab and Linux/Mac(?))\n",
    "!sed -i.bak 1i\"Source,Target,Weight\" ./edgelist.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netscience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8d6fe78df442a2821680c44ba2bb6e12dd7f31a25202dbe2c5c118e24ea90652"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

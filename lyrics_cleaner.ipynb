{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83371a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\gooog\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gooog\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gooog\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, wordnet, stopwords\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62c6949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lemmatize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_to_wordnet(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "    \n",
    "# list with stopwords and punctuation to remove\n",
    "stoplist = set(stopwords.words('english') + list(punctuation))\n",
    "\n",
    "def clean_lyrics(lyrics):\n",
    "    # change everything to lower case\n",
    "    lyrics = lyrics.lower()\n",
    "    # remove numbers\n",
    "    lyrics_nonum = re.sub(r'\\d+', '', lyrics)\n",
    "    \n",
    "    #tokenize the lyrics and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(lyrics_nonum))  \n",
    "    \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_to_wordnet(x[1])), nltk_tagged)\n",
    "    lemmatized_lyrics = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_lyrics.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_lyrics.append(lemmatizer.lemmatize(word, tag))\n",
    "            \n",
    "    unique_tokens = unique_tokens = list(set(lemmatized_lyrics))\n",
    "    \n",
    "    # remove stopwords\n",
    "    unique_nostop = [word for word in unique_tokens if word not in stoplist]\n",
    "    return unique_nostop\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09b7312e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing year 1955...\n",
      "                                title               artist  \\\n",
      "0  (Its Been A Long Time) Pretty Baby                 Gino   \n",
      "0                     A Certain Smile        Johnny Mathis   \n",
      "0           All I Have To Do Is Dream  The Everly Brothers   \n",
      "0                          Angel Baby          Dean Martin   \n",
      "0                 Are You Really Mine       Jimmie Rodgers   \n",
      "\n",
      "                                              lyrics  \\\n",
      "0  time open get tryin also 's eakup cryin 've do...   \n",
      "0  many fleeting exactly know also lead come sudd...   \n",
      "0  need arm lip also mine dream away tight whenev...   \n",
      "0  'round flip also start angel mine dream clover...   \n",
      "0  also fantastic know mine part onlyembed 'm hof...   \n",
      "\n",
      "                                            filename  \n",
      "0     (Its Been A Long Time) Pretty Baby by Gino.txt  \n",
      "0               A Certain Smile by Johnny Mathis.txt  \n",
      "0  All I Have To Do Is Dream by The Everly Brothe...  \n",
      "0                      Angel Baby by Dean Martin.txt  \n",
      "0          Are You Really Mine by Jimmie Rodgers.txt  \n",
      "\n",
      "Lemmatizing year 1956...\n",
      "                                title               artist  \\\n",
      "0  (Its Been A Long Time) Pretty Baby                 Gino   \n",
      "0                     A Certain Smile        Johnny Mathis   \n",
      "0           All I Have To Do Is Dream  The Everly Brothers   \n",
      "0                          Angel Baby          Dean Martin   \n",
      "0                 Are You Really Mine       Jimmie Rodgers   \n",
      "\n",
      "                                              lyrics  \\\n",
      "0  time open get tryin also 's eakup cryin 've do...   \n",
      "0  many fleeting exactly know also lead come sudd...   \n",
      "0  need arm lip also mine dream away tight whenev...   \n",
      "0  'round flip also start angel mine dream clover...   \n",
      "0  also fantastic know mine part onlyembed 'm hof...   \n",
      "\n",
      "                                            filename  \n",
      "0     (Its Been A Long Time) Pretty Baby by Gino.txt  \n",
      "0               A Certain Smile by Johnny Mathis.txt  \n",
      "0  All I Have To Do Is Dream by The Everly Brothe...  \n",
      "0                      Angel Baby by Dean Martin.txt  \n",
      "0          Are You Really Mine by Jimmie Rodgers.txt  \n",
      "\n",
      "Lemmatizing year 1957...\n",
      "                                title               artist  \\\n",
      "0  (Its Been A Long Time) Pretty Baby                 Gino   \n",
      "0                     A Certain Smile        Johnny Mathis   \n",
      "0           All I Have To Do Is Dream  The Everly Brothers   \n",
      "0                          Angel Baby          Dean Martin   \n",
      "0                 Are You Really Mine       Jimmie Rodgers   \n",
      "\n",
      "                                              lyrics  \\\n",
      "0  time open get tryin also 's eakup cryin 've do...   \n",
      "0  many fleeting exactly know also lead come sudd...   \n",
      "0  need arm lip also mine dream away tight whenev...   \n",
      "0  'round flip also start angel mine dream clover...   \n",
      "0  also fantastic know mine part onlyembed 'm hof...   \n",
      "\n",
      "                                            filename  \n",
      "0     (Its Been A Long Time) Pretty Baby by Gino.txt  \n",
      "0               A Certain Smile by Johnny Mathis.txt  \n",
      "0  All I Have To Do Is Dream by The Everly Brothe...  \n",
      "0                      Angel Baby by Dean Martin.txt  \n",
      "0          Are You Really Mine by Jimmie Rodgers.txt  \n",
      "\n",
      "Lemmatizing year 1958...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m source \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path_songs, song), \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcp1252\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m lyrics \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39mread()\n\u001b[1;32m---> 18\u001b[0m clean_tokens \u001b[39m=\u001b[39m clean_lyrics(lyrics)\n\u001b[0;32m     19\u001b[0m lyrics_string \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(clean_tokens)\n\u001b[0;32m     20\u001b[0m song_lyrics\u001b[39m.\u001b[39mappend(lyrics_string)\n",
      "Cell \u001b[1;32mIn [3], line 28\u001b[0m, in \u001b[0;36mclean_lyrics\u001b[1;34m(lyrics)\u001b[0m\n\u001b[0;32m     25\u001b[0m lyrics_nonum \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, lyrics)\n\u001b[0;32m     27\u001b[0m \u001b[39m#tokenize the lyrics and find the POS tag for each token\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m nltk_tagged \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mpos_tag(nltk\u001b[39m.\u001b[39;49mword_tokenize(lyrics_nonum))  \n\u001b[0;32m     30\u001b[0m \u001b[39m#tuple of (token, wordnet_tag)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m wordnet_tagged \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: (x[\u001b[39m0\u001b[39m], nltk_to_wordnet(x[\u001b[39m1\u001b[39m])), nltk_tagged)\n",
      "File \u001b[1;32mc:\\Users\\gooog\\anaconda3\\envs\\netscience\\lib\\site-packages\\nltk\\tag\\__init__.py:166\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[39mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39mtag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    165\u001b[0m tagger \u001b[39m=\u001b[39m _get_tagger(lang)\n\u001b[1;32m--> 166\u001b[0m \u001b[39mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32mc:\\Users\\gooog\\anaconda3\\envs\\netscience\\lib\\site-packages\\nltk\\tag\\__init__.py:123\u001b[0m, in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtokens: expected a list of strings, got a string\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 123\u001b[0m     tagged_tokens \u001b[39m=\u001b[39m tagger\u001b[39m.\u001b[39;49mtag(tokens)\n\u001b[0;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m tagset:  \u001b[39m# Maps to the specified tagset.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         \u001b[39mif\u001b[39;00m lang \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\gooog\\anaconda3\\envs\\netscience\\lib\\site-packages\\nltk\\tag\\perceptron.py:187\u001b[0m, in \u001b[0;36mPerceptronTagger.tag\u001b[1;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tag:\n\u001b[0;32m    186\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_features(i, word, context, prev, prev2)\n\u001b[1;32m--> 187\u001b[0m     tag, conf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(features, return_conf)\n\u001b[0;32m    188\u001b[0m output\u001b[39m.\u001b[39mappend((word, tag, conf) \u001b[39mif\u001b[39;00m return_conf \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m (word, tag))\n\u001b[0;32m    190\u001b[0m prev2 \u001b[39m=\u001b[39m prev\n",
      "File \u001b[1;32mc:\\Users\\gooog\\anaconda3\\envs\\netscience\\lib\\site-packages\\nltk\\tag\\perceptron.py:66\u001b[0m, in \u001b[0;36mAveragedPerceptron.predict\u001b[1;34m(self, features, return_conf)\u001b[0m\n\u001b[0;32m     64\u001b[0m     weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights[feat]\n\u001b[0;32m     65\u001b[0m     \u001b[39mfor\u001b[39;00m label, weight \u001b[39min\u001b[39;00m weights\u001b[39m.\u001b[39mitems():\n\u001b[1;32m---> 66\u001b[0m         scores[label] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m value \u001b[39m*\u001b[39m weight\n\u001b[0;32m     68\u001b[0m \u001b[39m# Do a secondary alphabetic sort, for stability\u001b[39;00m\n\u001b[0;32m     69\u001b[0m best_label \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m label: (scores[label], label))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for year in range(1955, 2024, 1):\n",
    "    path_songs = f\"./songs/songs_{year}\"\n",
    "    songlist = [song for song in os.listdir(path_songs) if os.path.isfile(os.path.join(path_songs, song))]\n",
    "    # print(songlist)\n",
    "    print(f\"Lemmatizing year {year}...\")\n",
    "    df_songs = pd.DataFrame(columns = ['title','artist'])\n",
    "    song_lyrics = []\n",
    "\n",
    "    for song in songlist:    \n",
    "        # getting title and artist from the file name\n",
    "        title_artist = pd.DataFrame(song[:-4].split(' by ',1))\n",
    "        title_artist = title_artist.transpose()\n",
    "        title_artist.columns = ['title','artist']\n",
    "            \n",
    "        # clean lyrics\n",
    "        source = open(os.path.join(path_songs, song), 'r', encoding='cp1252')\n",
    "        lyrics = source.read()\n",
    "        clean_tokens = clean_lyrics(lyrics)\n",
    "        lyrics_string = ' '.join(clean_tokens)\n",
    "        song_lyrics.append(lyrics_string)\n",
    "        \n",
    "        # add to the dataframe\n",
    "        df_songs = pd.concat([df_songs, title_artist])\n",
    "\n",
    "    df_songs['lyrics'] = song_lyrics\n",
    "    df_songs['filename'] = songlist\n",
    "\n",
    "    print(df_songs.head())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5caaa48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netscience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8d6fe78df442a2821680c44ba2bb6e12dd7f31a25202dbe2c5c118e24ea90652"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
